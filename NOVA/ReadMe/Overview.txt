üöó Voice-Based Driver Assistant (DMS Assistant Module)
This project is a real-time voice-controlled driver assistant module developed as part of a Driver Monitoring System (DMS). It enables hands-free interaction through wake-word detection, voice command handling, driver status monitoring, and emergency escalation using TTS/STT, real-time JSON polling, and a multimodal feedback system.

---------------------------üì¶ System Overview üì¶ --------------------------
The assistant runs on a GPU laptop and includes:

Voice interaction (STT/TTS) using Whisper-v3 via Groq API and edge-tts.

Wake-word detection via continuous passive listening.

Emergency checks with fallback logic and contact escalation.

Visual feedback using eel and Bootstrap-styled HTML frontend.

Real-time monitoring of JSON alerts (from Jetson Nano or another AI model source).

-----------------------------üß† Architecture üß†--------------------------------
==========================
üß± Core Components
==========================
(Class)                  (Purpose)
AppState ------------->	Global object storing system mode, flags, file paths, etc.
AudioManager --------->	Handles TTS via edge-tts, plays alarms and messages.
UserManager ---------->	Records audio, checks wake words, handles alerts.
LLMManager -----------> Handles voice command response via local LLM (Ollama).

==========================
üß≠ Flow Summary
==========================
The system operates in two main modes: Monitoring Mode and Assistant Mode, with seamless transitions based on driver status or voice commands.

1) Monitoring Mode (Default Startup Mode)
When the system starts, it enters Monitoring Mode by default. In this state, it continuously polls a real-time JSON file (fetched via ngrok) generated by driver behavior detection models (e.g., fatigue, gaze, hands on wheel).

	a)Key Behaviors:
		If critical alerts are detected‚Äîsuch as:
			Fatigue_Alert = on
            Sleep_Alert = on
			Hands off wheel continuously for a preset duration
		‚Üí The system initiates a verbal check-up by saying:
		"Hey, are you okay? Can you hear me?"

	b) It waits for the driver's response within a defined timeout. If no response is detected, it:
       Repeats the verbal check-up once more.

    c) If still no response, it activates the emergency protocol, which includes:

		-Triggering a buzzer alert
		-Sending a WhatsApp message to the driver‚Äôs emergency contact, containing:
		-Live in-car video stream
		-Real-time GPS location
		-Sending a backup email alert with the same information

    d) If no emergency condition is detected, the system forwards the full JSON status to the LLM (LLaMA 3.2) to generate short, polite safety tips, e.g.:

		"Please avoid drinking while driving to ensure your safety."

	e) The monitoring feature can be manually enabled/disabled:

		-via screen buttons
		-via voice commands such as:"enable monitoring" or "disable monitoring"

2) Assistant Mode (Driver-Initiated)
Assistant Mode allows the driver to interact with the system conversationally, similar to a smart assistant.

		a) How it‚Äôs triggered:
			The driver presses the mic button on the dashboard UI Or simply says: ‚ÄúHello Nova‚Äù

		b) What it does:
			-Transitions the system from monitoring to an active listening state
			-Records the user‚Äôs query and transcribes it using Whisper v3 (via Groq)
			-Forwards the transcribed question to LLaMA 3.2, a local large language model, to generate a response
			-Responds verbally and visually via: Text-to-speech using edge-tts


This mode supports general queries (e.g., ‚ÄúHow far is Dokki?‚Äù or ‚Äúexplain to me ...‚Äù), and maintains a lightweight context of the conversation to provide relevant responses.



----------------------------üêç Backend Stack (Python 3.x) üêç-------------------------

Library        | Purpose
---------------|---------------------------------------------------------------
eel            | Connects Python and JavaScript. Lets Python call JS and vice versa.
sounddevice    | Records audio from the mic as NumPy arrays.
scipy.io.wav   | Writes audio buffers to .wav files.
pydub          | Analyzes audio levels to detect silence.
requests       | Fetches JSON alert files from the remote ngrok-exposed server.
json           | Saves/loads system states (flags, alerts, location).
geopy          | Converts lat/lon into full street addresses.
groq           | Sends audio to Whisper-v3 API for transcription (STT).
ollama         | Uses LLaMA 3.2 model to generate intelligent replies.
edge-tts       | Text-to-speech engine using Microsoft's Edge voicess.